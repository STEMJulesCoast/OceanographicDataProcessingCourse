{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<p style=\"color:red;\">If you are using Binder instead of downloading the repository and working locally, this script might not run due to the relatively large datasets and Binder's limited resources. I recommend working locally on your own computer for optimal performance.</p>**\n",
    "\n",
    "\n",
    "\n",
    "# Transitioning from Surface Data to Vertical Profiles\n",
    "\n",
    "In previous sessions, we focused on surface data, particularly wind and sea surface temperature (SST). Now, we transition to analyzing vertical profiles using Argo data, which provide temperature and  salinity and their uncertainty at various depths. Unlike the SST data from Session 2, which was accessed via OPeNDAP, here we work with objectively mapped Argo data, downloaded over HTTPS. We will download, process, and combine these datasets, incorporating error handling and logical conditions such as if-else statements to ensure robustness.\n",
    "\n",
    "\n",
    "[Argo floats](https://www.marine.ie/site-area/infrastructure-facilities/marine-research-infrastructures/argo-network): autonomous instruments that drift with ocean currents and periodically dive to various depths. On their way to the surface, they measure temperature, salinity, and depth. \n",
    "\n",
    "\n",
    "**<p style=\"color:royalblue;\">A brief exploration of gridding techniques for in situ data can be found in the notebook `IntroGridding.ipynb`.</p>**\n",
    "\n",
    "In this session, we will explore both horizontal and vertical patterns of temperature, salinity, and depth across different ocean basins. You will calculate and analyze mixed layer depth and calculate parameters averaged over the mixed layer and assess whether surface measurements can serve as reliable proxies for mixed layer processes.\n",
    "\n",
    "Run the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os\n",
    "import requests \n",
    "import zipfile\n",
    "import xarray as xr \n",
    "import numpy as np \n",
    "import gsw\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import holoviews as hv\n",
    "#from IPython.display import Image\n",
    "import hvplot.xarray\n",
    "hv.extension('bokeh')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the necessary libraries, we load the Argo data from EN4 using the HTTPS protocol. These datasets are available via the [Met Office website](https://www.metoffice.gov.uk/hadobs/en4/download-en4-2-2.html). We select the objectively analyzed data, which have been error-corrected and include uncertainty information. In the following code, we use a .txt file that contains the list of datasets we want to download. The code requests the datasets and uses if-else statements to manage the download process, ensuring that files are downloaded - if necessary, unzipped - , and processed. Additionally, the code now checks if the zip files already exist, in which case they are simply read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "\n",
    "# Define the folder where files will be saved and unzipped\n",
    "save_directory = '../Data/EN4'\n",
    "# creates the save_directory, if it not exists\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Read the list of URLs from the download list file\n",
    "with open('../Data/EN4/EN.4.2.2.analyses.g10.download-list.txt') as f:\n",
    "    urls = f.readlines()\n",
    "\n",
    "# List to store the paths of extracted NetCDF files\n",
    "nc_files = []\n",
    "\n",
    "# Loop through each URL, download, unzip, and collect NetCDF file paths\n",
    "for url in urls:\n",
    "    url = url.strip()  # Clean up the URL\n",
    "    if url:  # Check if the URL is not empty\n",
    "        file_name = url.split('/')[-1].strip()\n",
    "        file_path = os.path.join(save_directory, file_name)\n",
    "        \n",
    "        # Check if the zip file already exists\n",
    "        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "            print(f\"{file_name} already exists. Skipping download.\")\n",
    "        else:\n",
    "            # Download the file\n",
    "            print(f\"Downloading {file_name} to {file_path}...\")\n",
    "            r = requests.get(url)\n",
    "        \n",
    "            if r.status_code == 200: # sucessful request\n",
    "                # Save the zip file\n",
    "                with open(file_path, 'wb') as file:\n",
    "                    file.write(r.content)\n",
    "                \n",
    "                # Check if the file exists and is not empty\n",
    "                if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "                    print(f\"{file_name} downloaded successfully.\")\n",
    "                else:\n",
    "                    print(f\"Failed to download {file_name}, file not found or empty.\")\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"Failed to download {file_name}, status code: {r.status_code}\")\n",
    "                continue\n",
    "        \n",
    "        # Unzip the file and collect NetCDF file paths\n",
    "        print(f\"Extracting {file_name}...\")\n",
    "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(save_directory)\n",
    "            for extracted_file in zip_ref.namelist():\n",
    "                if extracted_file.endswith('.nc'):\n",
    "                    print(f\"Found {extracted_file}\")\n",
    "                    nc_files.append(os.path.join(save_directory, extracted_file))\n",
    "\n",
    "# Use open_mfdataset to combine all NetCDF files into a single dataset\n",
    "if nc_files:\n",
    "    ds_en4 = xr.open_mfdataset(nc_files, combine='by_coords')\n",
    "    print(ds_en4)\n",
    "else:\n",
    "    print(\"No datasets downloaded or processed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the connection becomes overloaded because we're all trying to download the data at the same time, please download the data manually using the links in the txt file and store them in the folder [../Data/EN4](../Data/EN4/). Then, run the cell above again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Exercise**:\n",
    "\n",
    "Explore the dataset you just loaded by answering the following questions:\n",
    "\n",
    "    What is the shape of the dataset?\n",
    "    Examine the dimensions and size of the dataset.\n",
    "\n",
    "    Which variables are contained within the dataset?\n",
    "    List the variables provided in the dataset and their descriptions.\n",
    "\n",
    "    What is the resolution of the data?\n",
    "    Analyze the spatial resolution of the dataset.\n",
    "\n",
    "    What units are the data presented in?\n",
    "    Investigate the units of measurement for the variables in the dataset.\n",
    "\n",
    "    \n",
    "\n",
    "Use the appropriate methods like e.g. .dims, .data_vars, and .attrs to extract this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to get the dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to get the variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code to get the horizontal resolution of the data (hint: use np.diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the attributes of a specific variable, for example, temperature.  \n",
    "Hint: Use `ds_en4['temperature'].attrs` to explore all attributes associated with this variable.\n",
    "\n",
    "Identify the attribute that corresponds to the unit (e.g., `'units'`).\n",
    "\n",
    "Use the `.get()` method to retrieve the value of the 'units' attribute:  \n",
    "`ds_en4['temperature'].attrs.get('units')`\n",
    "\n",
    "Consider writing a loop to retrieve the units for all variables in the dataset.  \n",
    "You can iterate through ds_en4.data_vars and access the `.attrs.get('units')` for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code to get the units of each variable \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is the vertical spacing between depth levels?\n",
    "\n",
    "Examine the differences between consecutive depth levels to understand the vertical structure of the dataset.\n",
    "\n",
    "Hint: You can again use `np.diff()` to calculate the differences between consecutive depth levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we plot the temperature.\n",
    "\n",
    "### Plotting with hvplot:\n",
    "\n",
    "We use hvplot because it allows us to explore multidimensional datasets interactively and efficiently. This is particularly useful for visualizing datasets with 3 or 4 dimensions (e.g., time, longitude, latitude, and depth). `hvplot` works directly with xarray, allowing the creation of interactive plots.\n",
    "\n",
    "For instance, when we visualize temperature using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "\n",
    "ds_en4.temperature.hvplot('lon', 'lat', width=700, height=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**2. Exercise**: \n",
    "\n",
    "Convert the temperature from Kelvin to Celsius and then visualize it using `hvplot`.   \n",
    "After converting the temperature, update the units in the dataset to reflect the change from Kelvin to Celsius. You can also customize the plot by setting color limits (`clim`) and choosing a different colormap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code and plot here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Depth Levels in EN4\n",
    "\n",
    "Unlike the datasets we have explored previously, the EN4 dataset includes data at various depth levels. This allows us to examine how variables such as temperature and salinity change not just horizontally but also vertically throughout the ocean.\n",
    "\n",
    "We create a vertical section plot. Specifically, we will plot a temperature section in the Pacific Ocean at 220°E (which is equivalent to 140°W)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "pacific_section = ds_en4.sel(lon = 220) # 220-360= -140 -> Pacific\n",
    "pacific_section['depth'] = -1 * pacific_section['depth']  # Invert depth\n",
    "pacific_section.temperature[:,:35,:].hvplot.quadmesh('lat', 'depth', clim=(0, 30), cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the Atlantic at 30°W:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "\n",
    "atlantic_section = ds_en4.sel(lon = 330) #330-360 = 30°W -> Atlantic\n",
    "atlantic_section['depth'] = -1 * atlantic_section['depth']  # Invert depth\n",
    "atlantic_section.temperature[:,:35,:].hvplot.quadmesh('lat', 'depth', clim=(0, 30), cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will modify the plot by using contour lines for visualization:\n",
    "\n",
    "This makes it easier to visually identify patterns and gradients in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATLANTIC\n",
    "# run the cell\n",
    "#with contours\n",
    "atlantic_section = ds_en4.sel(lon=330)  # 30°W\n",
    "atlantic_section['depth'] = -1 * atlantic_section['depth']  # Invert depth\n",
    "\n",
    "# Plot filled temperature contours\n",
    "atlantic_section.temperature[:,:35,:].hvplot.contourf(\n",
    "    x='lat', \n",
    "    y='depth', \n",
    "    levels=30,  # Number of contour levels\n",
    "    cmap='RdBu_r',  # Reversed red-blue colormap\n",
    "    clim=(-2, 30), \n",
    "    width=700, \n",
    "    height=400, \n",
    "    title='Atlantic Temperature Filled Contours at 30°W'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACIFIC\n",
    "# run the cell\n",
    "# with contours \n",
    "# Plot filled temperature contours\n",
    "pacific_section.temperature[:,:35,:].hvplot.contourf(\n",
    "    x='lat', \n",
    "    y='depth', \n",
    "    levels=30,  # Number of contour levels\n",
    "    cmap='RdBu_r',  # Reversed red-blue colormap\n",
    "    clim=(-2, 30), \n",
    "    width=700, \n",
    "    height=400, \n",
    "    title='Pacific Temperature Filled Contours at 140°W'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Observations\n",
    "\n",
    ".....\n",
    "\n",
    "**Extra Exercise:** Analyse Salinity\n",
    "\n",
    "### Connecting Back to Session 2 and Introducing the Vertical Section - ENSO\n",
    "In Session 2, we analyzed sea surface temperature (SST) data over an extended period and calculated both anomalies and ENSO composites. If you're not familiar with this, I recommend revisiting Session 2 and reviewing the exercises.\n",
    "\n",
    "We observed that temperature anomalies during ENSO events exhibit significant amplitude in the central Pacific, particularly around the equator. \n",
    "\n",
    "\n",
    "To better understand this, let’s first take a look at the next diagram, which shows the variation of surface temperatures over time and longitude along the equator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "\n",
    "enso_section = ds_en4.sel(lat =0,lon = slice(200,260)) #160°W-100°W\n",
    "enso_section['depth'] = -1 * enso_section['depth']  # Invert depth (we need this later for the section plot)\n",
    "\n",
    "\n",
    "# Select the first depth level for the Hovmöller plot\n",
    "first_depth_level = enso_section.isel(depth=0)  # First available depth level\n",
    "\n",
    "# Create a Hovmöller plot: time vs longitude at the equator for the first depth level\n",
    "\n",
    "hovmoeller_plot = first_depth_level.temperature.hvplot.quadmesh('time', 'lon', \n",
    "                                                                           clim=(25, 30), cmap='coolwarm', \n",
    "                                                                           width=700, height=400, invert = True) # invert=True reverses the values along the y-axis (e.g., time runs from bottom to top)\n",
    "\n",
    "# Display the plot\n",
    "hovmoeller_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is called a **Hovmöller** diagram an it is very often used in oceanography/climatology.  \n",
    "\n",
    "In the Hovmöller diagram, which shows absolute temperatures at the surface along the equator, we can already observe differences in temperature across time and longitude. These differences could be linked to the El Niño-Southern Oscillation (ENSO) cycle.\n",
    "\n",
    "Temperature Gradients: Notice how the western Pacific generally has warmer surface temperatures than the eastern Pacific. This persistent gradient reflects somehow the typical climate state of the tropical Pacific, where warm water piles up in the west due to the trade winds.\n",
    "\n",
    "ENSO Influence: During El Niño events, the warm water that is usually confined to the western Pacific shifts eastward, resulting in warmer-than-usual temperatures across the central and eastern Pacific. Conversely, during La Niña events, cold upwelling in the eastern Pacific intensifies, leading to a stronger east-west temperature gradient with colder temperatures in the east and warmer temperatures in the west.\n",
    "\n",
    "\n",
    "\n",
    "### But what happens below the surface? How do these patterns extend vertically through the water column?\n",
    "\n",
    "In the following section, we will take a closer look at the vertical temperature structure along the central Pacific, from 160°W to 100°W, to explore how ENSO impacts not just the surface but the entire vertical profile of the ocean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "\n",
    "enso_section.temperature[:,:20,:].hvplot.contourf(\n",
    "    x='lon', \n",
    "    y='depth', \n",
    "    levels=10,  # Number of contour levels\n",
    "    cmap='coolwarm',  # Reversed red-blue colormap\n",
    "    clim=(10, 30), \n",
    "    width=700, \n",
    "    height=400, \n",
    "    title='Pacific Temperature Filled Contours at the equator'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining the Temperature Section and ENSO Influence\n",
    "\n",
    "When plotting this temperature section along the equator, we can observe differences between the temperatre especially between 2022 and 2023. This region is strongly influenced by ENSO (El Niño-Southern Oscillation) events. During this period, several La Niña events were followed by an El Niño in 2023, and this is reflected in the temperature patterns.\n",
    "\n",
    "One notable feature is that surface temperatures are generally higher in the western Pacific, while the thermocline (the boundary between warm surface water and colder deep water) is tilted. In the west, the warm water extends much deeper than in the east.\n",
    "\n",
    "This tilting of the thermocline is a classic feature of ENSO. During La Niña events, the easterly trade winds strengthen, pushing water toward the western Pacific and deepening the thermocline there, while in the eastern Pacific, upwelling brings colder water to the surface. Conversely, during El Niño events, these trade winds weaken or reverse, causing the warm water to shift eastward and leads to higher surface temperatures in the central and eastern Pacific.\n",
    "\n",
    "In summary, the tilting thermocline and the warm water depth differences could be related to the ENSO cycle, reflecting the dynamic interplay between ocean temperatures and atmospheric wind patterns in this region.\n",
    "\n",
    "The following picture from [US National Weather Service](https://www.cpc.ncep.noaa.gov/products/analysis_monitoring/ensocycle/lanina_schem.shtml) briefly describes the La Nina phenomenon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.cpc.ncep.noaa.gov/products/analysis_monitoring/ensocycle/djfschem_lanina.gif\" width=\"500\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run the cell\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "# How to display an image from a URL\n",
    "Image(url='https://www.cpc.ncep.noaa.gov/products/analysis_monitoring/ensocycle/djfschem_lanina.gif', width=500, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density, Mixed Layer Depth, Mixed Layer Salinity and Temperature \n",
    "\n",
    "Temperature is a key factor in determining density, alongside with salinity and pressure. Warmer water is less dense than colder water, leading to a stratified water column. \n",
    "\n",
    "Next, we’ll explore how these temperature differences impact the density and MLD in the Pacific.\n",
    "\n",
    "We will use the GSW (Gibbs SeaWater) library to calculate pressure, potential temperature, and potential density from the available temperature, depth, and salinity data. Further information: https://www.teos-10.org/pubs/gsw/html/gsw_contents.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell \n",
    "\n",
    "# Calculate pressure based on depth\n",
    "depth = ds_en4['depth']\n",
    "lat = ds_en4['lat']\n",
    "pressure = gsw.p_from_z(-depth, lat) # p varies with depth and geographical location\n",
    "\n",
    "# Calculate potential temperature ( temp of a water parcel when its moved adiabatically to the surface pressure) and density\n",
    "temp = ds_en4['temperature']\n",
    "sal = ds_en4['salinity'] \n",
    "# potential temperature:\n",
    "ptemp = gsw.pt0_from_t(sal, temp, pressure) \n",
    "\n",
    "# potential density:\n",
    "sigma0 = gsw.sigma0(sal, ptemp)\n",
    "\n",
    "# Set correct attributes for the potential density\n",
    "sigma0.name = 'Potential Density'\n",
    "sigma0.attrs['long_name'] = 'Potential Density (Sigma_0)'\n",
    "sigma0.attrs['units'] = 'kg m-3'\n",
    "\n",
    "# Remove unwanted attributes that come from the original salinity data (pop is used to remove specific attributes)\n",
    "sigma0.attrs.pop('valid_min', None)\n",
    "sigma0.attrs.pop('valid_max', None)\n",
    "sigma0.attrs.pop('standard_name', None)\n",
    "\n",
    "# Set correct attributes for the potential temperature\n",
    "\n",
    "ptemp.name = 'Potential Temperature'\n",
    "ptemp.attrs['long_name'] = 'Potential Temperature'\n",
    "ptemp.attrs['units'] = '°C'\n",
    "\n",
    "# Remove unwanted attributes that come from the original salinity data\n",
    "ptemp.attrs.pop('valid_min', None)\n",
    "ptemp.attrs.pop('valid_max', None)\n",
    "ptemp.attrs.pop('standard_name', None)\n",
    "\n",
    "\n",
    "# add variables to dataset\n",
    "ds_en4['ptemp'] = ptemp\n",
    "ds_en4['sigma0'] = sigma0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the density section at the same location as the previous temperature section at the equator. When viewing the density section, corresponding patterns to temperature become apparent, indicating that temperature changes could have a dominant impact on density variations in the ENSO region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell to see the plot\n",
    "sigma_enso = sigma0.sel(lat =0,lon = slice(200,260)) #160°W-100°W\n",
    "sigma_enso['depth'] = -1 * sigma_enso['depth']  # Invert depth\n",
    "\n",
    "# plot the first 20 depth levels (you can experiment with the number of depth levels)\n",
    "sigma_enso[:,:20,:].hvplot.contourf(\n",
    "    x='lon', \n",
    "    y='depth', \n",
    "    levels=20,  # Number of contour levels\n",
    "    cmap='coolwarm',  # Reversed red-blue colormap\n",
    "    clim=(21, 27), \n",
    "    width=700, \n",
    "    height=400, \n",
    "    title='Pacific Density Filled Contours at the equator'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Layer Depth (MLD)\n",
    "\n",
    "The Mixed Layer (ML) is the uppermost layer of the ocean, where properties like temperature and salinity are relatively uniform due to mixing processes etc. The MLD represents the depth at which the mixed layer transitions to the more stratified layer below, typically marked by a significant change in density, temperature, or salinity.\n",
    "\n",
    "ENSO events can cause the MLD to shift, as the position of the pycnocline (which is where the water density changes rapidly with depth) changes in response to e.g. warming or cooling of the surface waters.\n",
    "\n",
    "\n",
    "For the MLD, we use a density threshold, which calculates the MLD based on a change in potential density (0.03 kg m-3) starting from 5 meters depth (the first depth layer). The mixed layer facilitates energy transfer from the air to the water and is also a zone where biological activity is generally enhanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "\n",
    "# select density of the uppermost layer as reference\n",
    "# We use isel to select by the index of a coordinate (e.g., depth=0 corresponds to the uppermost layer).\n",
    "# Otherwise use sel when selecting based on the exact value of a coordinate (e.g., depth=5 for 5 m).\n",
    "surface_density = sigma0.isel(depth=0)\n",
    "\n",
    "# Calculate the density difference between the surface and each depth level\n",
    "density_diff = sigma0 - surface_density\n",
    "\n",
    "# Define a density threshold for MLD calculation (0.03 kg/m³)\n",
    "density_threshold = 0.03\n",
    "\n",
    "# Create a mask that is True (or 1) where density difference exceeds the threshold\n",
    "mask = density_diff >= density_threshold\n",
    "\n",
    "# Use the mask to find the minimum depth where the density difference meets the threshold, indicating the mld\n",
    "mld_from_surface_difference = depth.where(mask).min(dim='depth')\n",
    "\n",
    "mld_from_surface_difference.attrs['long_name'] = 'mixed layer depth based on the surface difference (threshold 0.03 kg m-3)'\n",
    "mld_from_surface_difference.attrs['units'] = 'm'\n",
    "# Add Mixed Layer Depth to the dataset\n",
    "ds_en4['mld'] = mld_from_surface_difference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Exercise: Plot the mean MLD over time. What do you observe?**\n",
    "\n",
    "Use `hvplot()`  for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual Amplitude and internannual variability \n",
    "\n",
    "**4. Exercise**:  \n",
    "Now we are interested in investigating where the mld exhibits a strong seasonal cycle and where interannual variability is more dominant.\n",
    "\n",
    "Your task is to first calculate the annual amplitude based on the climatological seasonal cycle and visualize it.\n",
    "\n",
    "Any ideas how we calculate the annual amplitude based on the climatology?\n",
    "\n",
    "Hints are given in the following code cell\n",
    "\n",
    "**<p style=\"color:royalblue;\">Given that we only have four years of data, the results may not be fully representative, but they can still provide useful insights. Think about how you would approach the calculation of the seasonal amplitude and how you would assess the interannual variability. Consider removing the seasonal cycle to isolate interannual fluctuations.</p>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to calculate the annual amplitude of MLD\n",
    "\n",
    "# Group data by month\n",
    "# monthly_climatology =\n",
    "\n",
    "# \n",
    "# Calculate the amplitude as the difference between the maximum and minimum monthly_climatology devided by 2\n",
    "# annual_amplitude = \n",
    "\n",
    "\n",
    "# Plot the amplitude \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Seasonal Variations to Analyze Remaining Variability**\n",
    "\n",
    "Now, remove the seasonal variations from the MLD data in order to focus on the remaining interannual variability. Even though we are working with only four years of data and not applying a detrending approach, this method helps isolate fluctuations beyond the regular seasonal cycle, providing insights into longer-term variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to calculate the interannual variability\n",
    "# Remove the seasonal cycle to calculate anomalies, you already calculated the monthly_climatology....\n",
    "#anomalies = \n",
    "\n",
    "\n",
    "\n",
    "# Calculate the standard deviation of the anomalies (interannual variability)\n",
    "# interannual_variability = \n",
    "\n",
    "\n",
    "# Plot interannual variability\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What do you observe?**  \n",
    "Where does the seasonal cycle dominate, and where are interannual changes pronounced?\n",
    "\n",
    "\n",
    "**<p style=\"color:royalblue;\">Keep in mind that we are analyzing only four years of data; longer time series would be better suited for analyzing interannual variability in detail.</p>**\n",
    "\n",
    "\n",
    "**5. Exercise**: \n",
    "1. Calculate the annual amplitude of your dataset `ds_en4` from the first depth layer and plot salinity. \n",
    "2. Average the salinity and temperature within the mixed layer. Then, calculate and plot the annual amplitude of the mixed layer salinity and temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to calculate the annual amplitude of ds_en4 and plot salinity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional: Plot the annual amplitude of temperature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average the `ds_en4` variables within the MLD, then calculate the annual amplitude and plot salinity.\n",
    "\n",
    "Tip: Use a mask to select only the data within the Mixed Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mixed layer salinity by averaging salinity up to the mixed layer depth (mld)\n",
    "\n",
    "# Hint: Mask values deeper than the mixed layer depth (mld) using .where()  and replace ...\n",
    "# ds_mld_masked = ds_en4.where(...)\n",
    "\n",
    "# Compute the mean within the mixed layer  (its a mean over depth)\n",
    "# mixed_layer_ds = \n",
    "\n",
    "# Group the mixed layer variables by month and compute the monthly climatology\n",
    "# This you know already from SST data \n",
    "#monthly_climatology = \n",
    "\n",
    "# Calculate the annual amplitude as the difference between the maximum and minimum monthly climatology divided by 2\n",
    "# This you have done before, scroll up and maybe copy relevant code lines\n",
    "# annual_amplitude_ds_mld =\n",
    "\n",
    "# Plot the annual amplitude of mixed layer salinity using hvplot\n",
    "#annual_amplitude_ds_mld['salinity'].hvplot(clim=(0, 1), cmap='plasma')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Debugging**:  \n",
    "Consider how you can confirm that only the data within the mixed layer depth (MLD) are included in the calculation. To verify this, you could plot the masked salinity data to visually check whether the masking based on the MLD was applied correctly. This can be useful for debugging and ensuring that only the intended data are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell, check if the variable name is correct\n",
    "\n",
    "ds_mld_masked['salinity'].hvplot('lon', 'lat', cmap='viridis', width=700, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use plot() and choose different time and depth\n",
    "\n",
    "ds_mld_masked['salinity'][1,3,:,:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Exercise** (for this session):\n",
    "\n",
    "Calculate and Plot the RMSE:\n",
    "\n",
    "- Calculate the RMSE between salinity in the first layer and the mixed layer salinity.\n",
    "- Repeat the same for temperature.\n",
    "- Plot both RMSE values on separate plots.\n",
    "\n",
    "The Root Mean Square Error (RMSE) is calculated using the following formula:\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (x_i - y_i)^2}\n",
    "$$\n",
    "​\n",
    "\n",
    "Where:\n",
    "\n",
    "$x_i$​ are the values from the first layer (e.g. salinity or temperature).  \n",
    "$y_i​$ are the values from the mixed layer.  \n",
    "$N$ is the number of data points.\n",
    "\n",
    "**<p style=\"color:royalblue;\">RMSE is typically used to measure the differences between predicted and observed (or true) values. In this case, we are using it to compare surface values (first layer) with mixed layer values, helping us assess the variability between these two layers.</p>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Calculate RMSE between variable at surface and variable averaged in the Mixed Layer\n",
    "\n",
    "# Compute the difference between your variables in the first layer and mixed layer averaged variables at each time step\n",
    "difference = \n",
    "\n",
    "# Square the differences\n",
    "squared_difference = \n",
    "\n",
    "# Compute the mean of the squared differences over time\n",
    "mean_squared_difference = \n",
    "\n",
    "# Take the square root to get the RMSE\n",
    "rmse = \n",
    "\n",
    "# Plot the RMSE to visualize where salinity is a better or worse proxy for mixed layer salinity\n",
    "rmse['salinity'].hvplot(clim=(0, .5), cmap='inferno', title='RMSE between Salinity and Mixed Layer Salinity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rmse of ptemp\n",
    "\n",
    "rmse['ptemp'].hvplot(clim=(0, .5), cmap='inferno', title='RMSE between Temperature and Mixed Layer Temperature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE provides a measure of the average difference between the values in the first layer and the mixed layer. A higher RMSE indicates greater variability or inconsistency between the two layers, suggesting that conditions in the first layer and the mixed layer differ significantly. Conversely, a lower RMSE indicates that the first layer and mixed layer have similar properties, meaning that, for example, salinity in the first layer is a good proxy for the mixed layer.\n",
    "\n",
    "This is particularly interesting because we often rely on satellite data due to their high spatial and temporal resolution, whereas in-situ data typically have more limited coverage. Satellites allow us to rapidly obtain global datasets, but the challenge is to determine whether surface measurements are sufficient to accurately represent the processes occurring in the mixed layer—the layer that is in direct contact and exchange with the atmosphere. Understanding this can help improve our ability to use surface satellite observations to infer subsurface processes in the ocean's mixed layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the cell to save the uppermost variables from ds_en4\n",
    "\n",
    "ds_en4[['salinity','sigma0','mld']].isel(depth=0).to_netcdf('../Data/EN4/EN4_uppermost_2020_2023.nc', engine='h5netcdf', compute=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the dataset and remove all .nc files from the EN4 folder to save space\n",
    "ds_en4.close()\n",
    "for file in nc_files:\n",
    "    os.remove(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
