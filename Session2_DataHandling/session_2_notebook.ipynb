{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling\n",
    "\n",
    "This seminar on Oceanographic Data Processing focuses on techniques in handling, analyzing, and visualizing large and complex datasets using Python. Key topics include:\n",
    "\n",
    "    Data Aggregation: Techniques to read and combine multiple datasets from various sources, enabling broader insights and pattern recognition.\n",
    "\n",
    "    Advanced Statistical Calculations: In-depth exploration of annual metrics.\n",
    "\n",
    "    Time Series Analysis: Understanding trends and cyclic patterns in time series data.\n",
    "\n",
    "    Automated Data Retrieval: Learning to automatically download data, including using OpenDAP for large datasets without specific access keys.\n",
    "\n",
    "    Multidimensional Data Manipulation: Expanding from monthly to annual and longer data analysis by organizing data into multidimensional arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "\n",
    "# Scientific libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.signal import detrend\n",
    "\n",
    "#Visualization libraries\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## datapath and filename\n",
    "datapath = '../Data/Wind'\n",
    "\n",
    "shapefile = '../Data/110m_cultural/ne_110m_admin_0_countries.shp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we're performing the following tasks:\n",
    "\n",
    "**File Paths for 2023 Wind Data:** We're setting up file paths for data corresponding to each month of the year 2023. For this we create a dictionary. When you start the 1. Exercise you will see the structure of a dictionary. There is also an example in the Python_cookbook.ipynb - Notebook.\n",
    "\n",
    "**Loading the Data:** Using [`xr.open_mfdataset()`](https://docs.xarray.dev/en/stable/generated/xarray.open_mfdataset.html), we're loading multiple netCDF datasets at once, which represent monthly data throughout 2023, and combining them by coordinates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "\n",
    "# Load data\n",
    "# Define file paths for the year 2023\n",
    "data_ranges = {year: list(range(1, 13)) for year in range(2023, 2024)}\n",
    "\n",
    "filenames = [os.path.join(datapath, f'cmems_obs-wind_glo_phy_my_l4_P1M_{year}{str(month).zfill(2)}.nc') \n",
    "              for year, months in data_ranges.items() for month in months]\n",
    "\n",
    "# Load all files for 2023 using open_mfdataset and combine them by coordinates (lat, lon)\n",
    "ds_2023 = xr.open_mfdataset(filenames, combine='by_coords') # mf = multible files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Exercise:**  \n",
    "To check the contents of `data_ranges`, `filenames`, and `ds_2023`, simply enter these variable names into the following code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Large Datasets with Dask in xarray\n",
    "\n",
    "\n",
    "The term `dask.array` in your xarray dataset indicates that Dask is being used for efficient computation and memory management. `open_mfdataset()` uses this by default to handle large datasets by loading them in chunks.\n",
    "\n",
    "    Dask Integration: Dask enables parallel and out-of-core computation, making it ideal for datasets too large to fit into memory.\n",
    "    Chunksize: The dataset is divided into chunks (e.g., 1 time step, 720 latitudes, 1440 longitudes), allowing for efficient processing by loading only necessary chunks.\n",
    "    Lazy Evaluation: Calculations are performed only when needed, improving efficiency for large datasets.\n",
    "\n",
    "\n",
    "In summary, your dataset is an xarray dataset with monthly wind data, and the use of Dask indicates that it's optimized for large-scale, possibly memory-intensive computations, with data stored in chunks and processed using parallel computing techniques.\n",
    "\n",
    "\n",
    "You successfully combined multible monthly data fields to a multidimensional xarray. Now, let's calculate wind speed variability for 2023 using `std()` to identify regions with the greatest variability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "\n",
    "# Calculate the wind speed from the eastward and northward components and the variability of wind speed using std()\n",
    "wind_speed_variability = ((ds_2023['eastward_wind']**2 + ds_2023['northward_wind']**2)**0.5).std(dim='time')\n",
    "\n",
    "# Plot the regions with the highest variability\n",
    "wind_speed_variability.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a plot with landmasses: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure and axis\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Define the map projection using Robinson (Basemap for Robinson projection)\n",
    "m = Basemap(projection='robin', lon_0=0, ax=ax)\n",
    "\n",
    "# Get longitude and latitude data from the dataset\n",
    "\n",
    "\n",
    "# Create a meshgrid for plotting\n",
    "lon2d, lat2d = np.meshgrid(wind_speed_variability.lon, wind_speed_variability.lat)\n",
    "\n",
    "# Transform the coordinates into the Robinson projection\n",
    "x, y = m(lon2d, lat2d)\n",
    "\n",
    "# Define levels for contouring (for eastward wind)\n",
    "levels = np.linspace(0, 4, 25)\n",
    "\n",
    "# Plot the eastward wind data using contourf\n",
    "cs = m.contourf(x, y, wind_speed_variability, cmap='viridis', levels=levels)\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = m.colorbar(cs, location='right', pad=\"10%\",ticks=np.arange(0, 4.1, .5))\n",
    "\n",
    "cbar.set_label(f'Wind Speed Variability (m s-1)')\n",
    "\n",
    "\n",
    "# Use Basemap to fill continents with white\n",
    "m.fillcontinents(color='white')\n",
    "\n",
    "# Draw coastlines (only coastlines, no rivers)\n",
    "m.drawcoastlines()\n",
    "\n",
    "# Add gridlines (parallels and meridians) for the Robinson projection\n",
    "m.drawparallels(np.arange(-90., 91., 30.), labels=[1, 0, 0, 0], linewidth=0.5, color='gray')\n",
    "m.drawmeridians(np.arange(-180., 181., 40.), labels=[0, 0, 0, 1], linewidth=0.5, color='gray')\n",
    "\n",
    "#\n",
    "# Set a title based on the dataset\n",
    "plt.title(f\"Wind Speed Variability (m s-1))\")\n",
    "# Save the figure with a transparent background\n",
    "#plt.savefig(\"wind_speed_variability_plot.png\", transparent=True)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop\n",
    "There are a few regions where wind variability is enhanced, such as the Indian Ocean. Let's explore which month has the highest mean wind speed.\n",
    "\n",
    "First, we'll slice the data to focus on the Indian Ocean region, as we have done in the last session. Then, we calculate the monthly wind speed using both the northward and eastward wind components. We'll **loop** through each month to average the wind data across the central Indian Ocean, giving us the mean wind speed for each month.\n",
    "\n",
    "Before calculating the mean wind speed for each month in the Indian Ocean, we first mask the wind data to ensure that land values do not affect our results. We use the `number_of_observations` variable in the dataset to create this mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell \n",
    "\n",
    "#'number_of_observations' is the variable used as a mask\n",
    "mask = ds_2023.number_of_observations.notnull()\n",
    "\n",
    "# Apply the mask to the eastward wind data\n",
    "ds_masked = ds_2023.where(mask)\n",
    "\n",
    "# Plot the masked eastward wind data\n",
    "ds_masked.eastward_wind[11,:,:].plot(cmap='coolwarm', figsize=(12, 5))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For calculating the mean values, we will now use the masked data. The region we are focusing on includes landmasses, and our dataset contains measurements for those areas. Including them in the calculations would distort the results, as we are only interested in the wind over the ocean. **Optional:** You can also test this yourself. First run the following cell and then use `ds_2023` instead of `ds_masked` in the calculations and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "\n",
    "\n",
    "\n",
    "# Define the region for the Indian Ocean\n",
    "# Indian Ocean (Latitudes: -20 to 20, Longitudes: 50 to 100)\n",
    "indian_ocean_wind = ds_masked.sel(lat=slice(-20, 20), lon=slice(50, 100))\n",
    "\n",
    "# List to store the mean wind speed for each month\n",
    "mean_wind_speeds = []\n",
    "\n",
    "# Loop through each month to calculate the mean wind speed in the Indian Ocean\n",
    "for month in range(1, 13): #-> 1:13 range() includes the start value but excludes the end value, so range(1, 13) returns numbers from 1 to 12\n",
    "    # Select data for the current month\n",
    "    monthly_data = indian_ocean_wind.sel(time=indian_ocean_wind['time.month'] == month)\n",
    "    \n",
    "    # Calculate wind speed\n",
    "    wind_speed = (monthly_data['eastward_wind']**2 + monthly_data['northward_wind']**2)**0.5\n",
    "    \n",
    "    # Calculate the mean wind speed over the region\n",
    "    # Since we are using Dask for parallel computation, we need to call compute() to finalize the calculation\n",
    "    mean_wind_speed = wind_speed.mean(dim=['lat', 'lon']).values#.compute().values \n",
    "    \n",
    "    # Append the mean wind speed for this month\n",
    "    mean_wind_speeds.append(mean_wind_speed)\n",
    "test = tesTest t\n",
    "# Create a time series using Pandas DataFrame\n",
    "months = pd.date_range(start='2023-01', periods=12, freq='MS')\n",
    "wind_speed_series = pd.DataFrame({'Month': months.strftime('%Y-%m'), 'Mean Wind Speed': mean_wind_speeds})\n",
    "\n",
    "# Plot the time series\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(wind_speed_series['Month'], wind_speed_series['Mean Wind Speed'], marker='o', linestyle='-', color='b')\n",
    "plt.title('Mean Wind Speed in the Indian Ocean (2023)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Mean Wind Speed (m s-1)')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the top 3 month with the highest mean wind speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell \n",
    "# Sort the DataFrame by 'Mean Wind Speed (m/s)' in descending order and select the top 3 months\n",
    "top_3_months = wind_speed_series.sort_values(by='Mean Wind Speed', ascending=False).head(3)\n",
    "\n",
    "# Output the result\n",
    "print(\"Top 3 months with the highest mean wind speed:\")\n",
    "print(top_3_months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\">Honestly, there's a much easier way to get the monthly data. I just wanted to show you another loop as an example of how you can approach this problem in different ways! Now, let’s move on to the more straightforward approach when analyzing xarray data.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "# Calculate wind speed\n",
    "wind_speed = (indian_ocean_wind['eastward_wind']**2 + indian_ocean_wind['northward_wind']**2)**0.5\n",
    "\n",
    "# Calculate the mean wind speed over lat and lon\n",
    "mean_wind_speed = wind_speed.mean(dim=['lat', 'lon']).compute()  \n",
    "# We used compute() to bring data into memory, argsort didn't work on Dask Arrays\n",
    "\n",
    "# Get the indices of the top 3 highest mean wind speeds\n",
    "top_3_indices = mean_wind_speed.argsort()[-3:][::-1].values  # Convert to numpy array and reverse\n",
    "\n",
    "# Extract the top 3 months and their corresponding wind speeds\n",
    "top_3_months = mean_wind_speed.isel(time=top_3_indices.tolist())  # Convert indices to list\n",
    "\n",
    "# Plot the time series\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mean_wind_speed['time'], mean_wind_speed.values, marker='o', linestyle='-', color='b')\n",
    "plt.title('Mean Wind Speed in the Indian Ocean (2023)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Mean Wind Speed (m s-1)')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "top_3_months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wind speeds are highest in the central Indian Ocean between July and September due to the influence of the southwest monsoon. During this period, strong winds blow from the ocean toward the Indian subcontinent, driven by temperature differences between the land and sea. This monsoon system brings higher wind speeds, particularly over the central and northern parts of the Indian Ocean, contributing to more intense weather patterns in the region.\n",
    "\n",
    "\n",
    "### Multidimensional Climate Data \n",
    "\n",
    "You might be wondering why we haven't yet analyzed oceanographic data. However, wind is a crucial climate variable that significantly impacts oceanographic processes such as currents and upwelling, which in turn affect sea temperature or salinity etc. Therefore, analyzing wind alongside other variables is a common practice in oceanography. Next, we will begin examining Sea Surface Temperature (SST) data, which we will access directly from NOAA using the OPeNDAP protocol (or from the HCU Cloud if the protocol encounters any issues or directly: https://downloads.psl.noaa.gov/Datasets/noaa.ersst.v5/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the cell\n",
    "#load SST data\n",
    "url = 'http://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/noaa.ersst.v5/sst.mnmean.nc'\n",
    "ds_sst = xr.open_dataset(url)\n",
    "ds_sst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If the URL from above didn't work, you can download the data via HCU Cloud:To run the code, please remove the # symbols before the lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##in case of OpeNDAP Problems\n",
    "##load SST data\n",
    "#import requests\n",
    "## Define the cloud link\n",
    "#url = 'https://cloud.hcu-hamburg.de/nextcloud/s/JmYoZ2zaXGQdWp6/download' \n",
    "\n",
    "## Download the file\n",
    "#filename = 'sst_data.nc'\n",
    "#response = requests.get(url)\n",
    "\n",
    "## Save the file locally\n",
    "#with open(filename, 'wb') as file:\n",
    "#    file.write(response.content)\n",
    "\n",
    "## Now open the file using xarray\n",
    "#ds_sst = xr.open_dataset(filename, use_cftime=True)\n",
    "#ds_sst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Exercise: Slice the data and select a more recent part of the data (e.g. 1960-2023)**\n",
    "\n",
    "Name the sliced data also `ds_sst`. We overwrite the loaded ds_sst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to examine whether the SST shows increased variability in the Indian Ocean. In the following steps, we will slice the SST data for the year 2023, focusing on the same region as we did for the wind data, and plot the time series.\n",
    "\n",
    "However, there are some differences compared to the wind data. For instance, the latitude in the SST dataset starts from the Northern Hemisphere and moves southward, whereas the wind data starts from the Southern Hemisphere. Additionally, the longitude in the SST dataset spans from 0 to 360 degrees, unlike the wind data, which uses the range from -180 to 180 degrees. For the Indian Ocean, the longitude remains the same, but we need to adjust the slicing for latitude accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the region for the Indian Ocean\n",
    "# Indian Ocean (Latitudes: -20 to 20, Longitudes: 50 to 100)\n",
    "indian_ocean_sst = ds_sst.sel(lat=slice(20, -20), lon=slice(50, 100),time = slice('2023','2024'))\n",
    "indian_ocean_sst['sst'].std(dim='time', skipna=True).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot also reveals higher variability near coastal areas and south of 10°S, where wind speed variability is also enhanced.\n",
    "\n",
    "We now want to calculate the mean SST for the Indian Ocean and plot the time series, similar to what we did for the wind data.\n",
    "\n",
    "**3. Exercise: Copy the code from above, where we calculated and plotted the mean Indian Ocean wind speed using `xarray`. Modify this code to calculate and plot the mean SST from `indian_ocean_sst` instead (without using loops or lists). Name it `mean_sst`.**\n",
    "\n",
    "Hint: Exclude the part with the `top_3_month` and just focus on calculating and plotting the mean values over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code and plot here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite interesting: SST shows its lowest values when wind speed is at its peak. To make this relationship clearer, let's plot both time series together. **Check, if your variable_names match, otherwise modify**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "\n",
    "# run the cell\n",
    "\n",
    "# Create the figure and the first axis (for SST)\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the SST data on the left y-axis\n",
    "ax1.set_xlabel('Month')\n",
    "ax1.set_ylabel('Mean SST (°C)', color='r')\n",
    "ax1.plot(mean_sst['time'], mean_sst['sst'].values, marker='o', linestyle='-', color='r', label='Mean SST')\n",
    "ax1.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "# Create a second y-axis for wind speed\n",
    "ax2 = ax1.twinx()  # This creates a second y-axis that shares the same x-axis\n",
    "ax2.set_ylabel('Mean Wind Speed (m/s)', color='b')\n",
    "ax2.plot(mean_wind_speed['time'], mean_wind_speed.values, marker='o', linestyle='-', color='b', label='Mean Wind Speed')\n",
    "ax2.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "# Set the title and layout\n",
    "plt.title('Mean SST and Wind Speed in the Indian Ocean (2023)')\n",
    "\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variabilities on various timescales\n",
    "\n",
    "The monsoon for example represents a clear seasonal signal in the Indian Ocean, which is also present in wind and sst. In climate science, analyzing seasonal trends is crucial for understanding the temporal dynamics of various climatic parameters, including sea surface temperature (SST). Seasonal variations in SST significantly impact weather patterns, marine ecosystems, and global climate systems. By examining seasonal means, we capture the typical state of the climate system at different times of the year, revealing patterns that may be obscured in longer-term averages.\n",
    "\n",
    "Seasonal means help highlight systematic changes caused by the Earth's tilt and orbit. Analyzing the standard deviation alongside the mean reveals how much variability exists during each season. High standard deviation indicates greater variability, while low values suggest more stable conditions.\n",
    "\n",
    "**Steps:** \n",
    "\n",
    "We calculate the seasonal means and standard deviations for the sea surface temperature (SST) data provided in the `ds_sst` dataset. This dataset contains monthly SST data over several decades.\n",
    "\n",
    "1. Calculation of the Seasonal Means: We group the data by the meteorological seasons (DJF for winter, MAM for spring, JJA for summer, and SON for autumn) and calculate the mean SST for each season using `groupby('time.season').mean('time')`.  \n",
    "\n",
    "2. Visualizing the Results: We create a set of plots to visualize the seasonal means. There will be be one plot for each season. We use a clear and contrast-rich colormap like `plasma.` Ensure that the variables in your calculations match those expected by the given plot code.  \n",
    "\n",
    "Note: The provided plot code offers a straightforward approach to plotting, ideal for an expeditious examination of results. We're bypassing the more intricate plot embellishments in favor of leveraging the integrated contour features of xarray, which utilizes Matplotlib's capabilities. This allows for a swift and efficient overview of the data without the additional complexity of extensive customization. However, it's worth noting that further enhancements can be made for more refined visual presentations, as demonstrated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the cell\n",
    "\n",
    "#calculation of seasonal means\n",
    "#seasonal_means = ds_sst['sst'].groupby('time.season').mean('time') # needs sometimes a lot of time-> chunk the dataset\n",
    "\n",
    "# Chunk the dataset for parallel processing\n",
    "ds_sst_chunked = ds_sst.chunk({'lat': 30, 'lon': 60})  # Adjust chunk size to optimize memory usage\n",
    "\n",
    "# Calculate seasonal means and trigger computation (Dask uses lazy evaluation, \n",
    "# but computing upfront may speed up later operations like plotting)\n",
    "seasonal_means = ds_sst_chunked['sst'].groupby('time.season').mean('time').compute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell, check variable names\n",
    "\n",
    "# Define the seasons order and titles for the plot\n",
    "seasons = ['DJF', 'MAM', 'JJA', 'SON']\n",
    "titles = ['Winter (DJF)', 'Spring (MAM)', 'Summer (JJA)', 'Autumn (SON)']\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10), constrained_layout=True)\n",
    "\n",
    "# Define the levels for contouring\n",
    "temperature_levels = np.arange(-2, 28.5, 0.5)\n",
    "\n",
    "for i, season in enumerate(seasons):\n",
    "    # Select the appropriate subplot\n",
    "    ax = axes.flatten()[i]\n",
    "    \n",
    "    # Plot the seasonal mean with a colormap designed for better visibility\n",
    "    im = seasonal_means.sel(season=season).plot(\n",
    "        ax=ax, add_colorbar=False, \n",
    "        cmap='plasma', levels=temperature_levels\n",
    "    )\n",
    "    \n",
    "    # Add contour lines to emphasize the differences\n",
    "    seasonal_means.sel(season=season).plot.contour(\n",
    "        ax=ax, colors='k', levels=temperature_levels, linewidths=0.5, alpha=0.5\n",
    "    )\n",
    "    \n",
    "    # Set the title to the current season\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "    # Adjust the colorbar\n",
    "    cbar = fig.colorbar(im, ax=ax, extend='both')\n",
    "    cbar.set_label('Sea Surface Temperature (°C)')\n",
    "\n",
    "    # Adjust the color scale limits\n",
    "    im.set_clim(-2, 30)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Exercise** \n",
    "1. Similar to the means, calculate the standard deviations for each season by grouping `ds_sst_chunked` by season and using `.std('time')` instead of `.mean('time')`. Visualize them using the provided plot code, adjusting the colorbar limitations and temperature levels accordingly to reflect the variability.\n",
    "\n",
    "2. Reflect on the Standard Deviations: Consider what the standard deviations might reveal and hypothesize what factors might contribute to the observed variability.\n",
    "\n",
    "Discuss the Findings: Once you have your plots, write a brief analysis discussing any noticeable patterns or differences between the seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your calculation of seasonal stds using ds_sst_chunked\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the code from plotting the mean and adjust temperature levels and rename seasonal_means_computed by seasonal_stds_computed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Exercise: Describe your Observations:**\n",
    "\n",
    "....Your observations here\n",
    "\n",
    "\n",
    "### Long-term Trend\n",
    "\n",
    "As we observed the seasonal standard deviations, you may have noticed increased variability not only in the mid-latitudes but also near the equatorial Pacific. This region is notably affected by the El Niño-Southern Oscillation (ENSO), a key driver of interannual climate variability.\n",
    "\n",
    "Climate variability occurs across different timescales. In addition to seasonal or interannual fluctuations, there is a long-term trend in sea surface temperature (SST) due to global warming. By understanding the long-term trend and variations on other timescales, we can better interpret the underlying processes driving these changes.\n",
    "\n",
    "In the following we calculate and plot the long-term trend, more precise, the trend per decade (period 1960-2023). Simply run the next cell and have a closer look.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "\n",
    "# Import the functions\n",
    "from trend_analysis import calculate_trend_per_decade, plot_trend_with_basemap\n",
    "\n",
    "# Assuming you have detrended SST data (sst_anom_detrended)\n",
    "# Calculate the trend per decade\n",
    "trend_decade = calculate_trend_per_decade(ds_sst.sst)\n",
    "\n",
    "plot_trend_with_basemap(trend_decade, vmin =-.5, vmax = .5, central_lon = 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same plot, but now focused on the Atlantic Ocean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trend_with_basemap(trend_decade, vmin =-.5, vmax = .5, central_lon = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to calculate the mean global SST trend per decade. Does this result seem realistic? **Search for relevant literature to verify your findings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "trend_decade.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Climatology and Anomalies\n",
    "\n",
    "In this exercise, we will calculate monthly climatology and anomalies. Anomalies represent deviations from the long-term monthly climatological mean and help us study e.g. interannual climate variability, such as ENSO events, by removing both the long-term trend and seasonal cycles.\n",
    "\n",
    "By calculating anomalies, we can better focus on the effects of events like El Niño and La Niña, which cause variations on interannual timescales.\n",
    "\n",
    "**6. Exercise: Calculate Monthly Climatology and Anomalies**\n",
    "\n",
    "- Climatology: Compute the long-term monthly averages (e.g., the average of all January values, February values, etc.). Use the `groupby('time.month')` function to group the data by month, then apply the `mean()` function along the time dimension to compute these monthly means.\n",
    "\n",
    "- Anomalies: Subtract the monthly climatology from each corresponding monthly value to calculate the anomalies. Use `groupby('time.month')` again to ensure that the months are aligned correctly before subtracting.\n",
    "\n",
    "Steps:\n",
    "\n",
    "- First, calculate the monthly climatology for the given time period (e.g., 1960–2023).\n",
    "- Next, compute the anomalies by subtracting the climatology from the original dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# sst_clim = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "#sst_anom = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You calculated monthly anomalies, which means the seasonal cycle has been removed. Now, we can focus on removing long-term trends from these anomalies. This step isolates variebilities on other timescales (e.g. interanual) by subtracting both the seasonal and long-term trends.\n",
    "\n",
    "Let's visualize the anomalies to see how the seasonal cycle has been removed, and then proceed with detrending to eliminate the long-term warming trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the cell\n",
    "\n",
    "lat_weights = np.cos(np.deg2rad(ds_sst.lat)).where(~sst_anom[0].isnull())\n",
    "lat_weights /= lat_weights.mean()\n",
    "\n",
    "#  Calculate the global mean of the original data (weighted by latitude)\n",
    "global_sst_original = (ds_sst['sst'] * lat_weights).mean(dim=['lon', 'lat'])\n",
    "\n",
    "# Calculate the global mean of the anomalies (weighted by latitude)\n",
    "global_sst_anom = (sst_anom * lat_weights).mean(dim=['lon', 'lat'])\n",
    "\n",
    "# Define the time range\n",
    "time_range = slice('1990', '2024')\n",
    "\n",
    "# Plot the original data and the anomalies using two y-axes\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the original SST data on the left y-axis\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Original SST (°C)', color='blue')\n",
    "ax1.plot(global_sst_original['time'].sel(time=time_range),global_sst_original.sel(time=time_range), label='Original SST', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Create a second y-axis for the anomalies\n",
    "ax2 = ax1.twinx()  # Create a second y-axis that shares the same x-axis\n",
    "ax2.set_ylabel('SST Anomalies (°C)', color='red')\n",
    "ax2.plot(global_sst_anom['time'].sel(time=time_range),global_sst_anom.sel(time=time_range), label='SST Anomalies (Seasonal Cycle Removed)', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "ax2.set_xlabel('Time')\n",
    "\n",
    "ax1.grid(True)  # Grid for the first axis\n",
    "\n",
    "# Set the title and layout\n",
    "plt.title('Comparison: Global Mean SST vs. Anomalies')\n",
    "fig.tight_layout()\n",
    "# Automatically format the x-axis for better display\n",
    "fig.autofmt_xdate()\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detrending\n",
    "\n",
    "Detrending removes long-term trends from the data. Before detrending, we check for missing values (NaNs) to ensure they occur only over land. NaN values within the data can cause problems when detrending, so we need to interpolate them. However, if NaNs are present only over land, we can simply replace them with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "\n",
    "# Count the number of NaNs per grid point across the time dimension\n",
    "nan_mask = sst_anom.isnull().sum(dim='time')\n",
    "\n",
    "# Plot the NaN mask to visualize where NaN values are present\n",
    "plt.figure(figsize=(10, 6))\n",
    "nan_mask.plot()\n",
    "plt.title('Number of NaN Values per Grid Cell')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We temporarily fill these NaNs with zeros for processing and later revert them back to NaNs to preserve the dataset's integrity.\n",
    "\n",
    "We use `apply_ufunc()` to apply the detrend function along the time axis, which removes any linear trends. Afterward, we ensure the original NaN positions are restored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "sst_anom_detrended = xr.apply_ufunc(detrend, sst_anom.fillna(0),\n",
    "                                    input_core_dims=[['time']],output_core_dims=[['time']] , dask='allowed').where(~sst_anom.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the detrended and raw SST anomaly time series averaged globally. Execute the following cell to generate a plot depicting the globally averaged SST anomalies over time. Ensure that the variable names in the code match those you have defined in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "\n",
    "# For a global average, we need to weigh the points by cosine of latitude.\n",
    "lat_weights = np.cos(np.deg2rad(ds_sst.lat)).where(~sst_anom[0].isnull())\n",
    "lat_weights /= lat_weights.mean()\n",
    "\n",
    "(sst_anom * lat_weights).mean(dim=['lon', 'lat']).plot(label='raw SST anomalies',color = 'b')\n",
    "(sst_anom_detrended * lat_weights).mean(dim=['lon', 'lat']).plot(label='detrended SST anomalies', color = 'r')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "# Add axis labels\n",
    "plt.xlabel('Time')  # x-axis is time\n",
    "plt.ylabel('Global Average SST Anomaly (°C)')  # Adjust the units if they're different. \n",
    "#Sea Surface Temperature (SST) anomalies are commonly reported in Kelvin (K), but it's also frequent to see them presented in degrees Celsius (°C) in the scientific literature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interannual Variabilities\n",
    "\n",
    "After calculating the monthly detrended anomalies, we can now examine how SST may vary during El Niño and La Niña events. To do this, we calculate composites, which involves identifying El Niño and La Niña periods and isolating the corresponding SST anomalies. Over a span of more than 30 years, we average the SST anomalies for both El Niño (positive phase) and La Niña (negative phase) events. For more details, refer to the functions in [`enso_functions.py`](enso_functions.py).  We'll explore functions and modules in more depth during the next session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "\n",
    "# Import the ENSO functions from the external script\n",
    "from enso_functions import calculate_nino34_index, calculate_composites, plot_composites\n",
    "\n",
    "# Assuming you have already loaded and detrended your SST anomaly dataset (sst_anom_detrended)\n",
    "\n",
    "# Step 1: Calculate the Niño3.4 index using the SST anomalies\n",
    "nino34_index = calculate_nino34_index(sst_anom_detrended)\n",
    "\n",
    "# Step 2: Calculate the SST composites for positive and negative ENSO events\n",
    "sst_anom_positive, sst_anom_negative = calculate_composites(sst_anom_detrended, nino34_index)\n",
    "\n",
    "# Step 3: Plot the SST composites\n",
    "plot_composites(sst_anom_positive, sst_anom_negative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Exercise: Discuss the plot of the composites with your neighbor. What patterns do you observe, and what might they indicate?**\n",
    "\n",
    "\n",
    "Further reading:   \n",
    "[El Niño/Southern Oscillation - Introduction](https://www.ncei.noaa.gov/access/monitoring/enso/)  \n",
    "[El Niño/Southern Oscillation - SST](https://www.ncei.noaa.gov/access/monitoring/enso/sst)   \n",
    "[Nino SST Indices (Nino 1+2, 3, 3.4, 4; ONI and TNI)](https://climatedataguide.ucar.edu/climate-data/nino-sst-indices-nino-12-3-34-4-oni-and-tni)  \n",
    "\n",
    "What are Composites:  \n",
    "Composites involve averaging data over specific conditions or events (like El Niño or La Niña). To calculate an ENSO composite, we first identify the periods of positive (El Niño) and negative (La Niña) phases based on a threshold value of the Niño 3.4 index. Then, we isolate the corresponding SST anomalies for those events and calculate the mean anomaly during those times. This allows us to compare the average SST patterns during El Niño and La Niña events, highlighting the spatial impacts of these climate phenomena.\n",
    "You can check the code example in [enso_functions.py](enso_functions.py), which demonstrates this, where we first calculate the Niño 3.4 index from the SST anomalies, identify positive and negative ENSO events, and then calculate and plot the composites.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
