{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating Frequency and Pattern Analysis: FFT and Wavelets and OOP\n",
    "\n",
    "At first glance, frequency analysis and object-oriented programming might not seem closely related – and that’s partially true. However, we will combine these approaches by integrating automated frequency analysis, such as Fourier Transformations and Wavelet Analysis, into a class structure, allowing us to streamline calculations for our datasets.\n",
    "\n",
    "Let’s start by loading the SST data, as we did before.\n",
    "\n",
    "Run the next cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import requests\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy.signal import detrend\n",
    "import pycwt as wavelet\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# Visualization\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import holoviews as hv\n",
    "\n",
    "import hvplot.xarray\n",
    "hv.extension('bokeh')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load SST (again) \n",
    "\n",
    "The next cell loads the data via OpenDAP if not stored locally.\n",
    "Note: If multiple users attempt to access the data simultaneously, it may cause performance issues or delays due to the limitations of the OpenDAP server. To avoid this, consider downloading the dataset locally if possible and not already done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the cell if data are not stored locally\n",
    "#load SST data\n",
    "\n",
    "url = 'http://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/noaa.ersst.v5/sst.mnmean.nc'\n",
    "ds_sst = xr.open_dataset(url)\n",
    "ds_sst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I stored data in [..Data/SST](../Data/SST/).  Make sure to check your path and filename accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load SST data from local storage\n",
    "filename = '../Data/SST/sst.mnmean.nc' \n",
    "ds_sst = xr.open_dataset(filename)\n",
    "ds_sst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Wind\n",
    "\n",
    "We also load wind data; these are CEMS wind data that we've previously worked with. However, this time they start from the year 2000 and are gridded on a 1° x 1° grid.\n",
    "\n",
    "**Question**: Do you remember how data can be resampled?\n",
    "\n",
    "\n",
    "\n",
    "The wind data can be downloaded directly from the cloud: [Wind Data](https://cloud.hcu-hamburg.de/nextcloud/s/GFzRHQaZPKxmZjY), or alternatively, by running the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the cloud link\n",
    "url = 'https://cloud.hcu-hamburg.de/nextcloud/s/GFzRHQaZPKxmZjY/download' \n",
    "\n",
    "## Download the file\n",
    "filename = '../Data/Wind/cmems_wind_coarse.nc'\n",
    "response = requests.get(url)\n",
    "\n",
    "## Save the file locally\n",
    "with open(filename, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "## Now open the file using xarray\n",
    "ds_wind = xr.open_dataset(filename, use_cftime=True)\n",
    "ds_wind\n",
    "\n",
    "# if its already stored locally you only need this:\n",
    "#ds_wind = xr.open_dataset('../Data/Wind/cmems_wind_coarse.nc',use_cftime=True) # change path if necessary\n",
    "#ds_wind\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Exercise:** We want to slice the SST data to match the time period of the wind data. Please perform this operation in the next cell. You can also load the data into memory using `load()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "ds_sst_24a = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to calculate the detrended anomalies again. Do you remember how?\n",
    "\n",
    "**2. Exercise**:  \n",
    "Calculate the detrended anomalies of the sliced SST data. We already covered this in Session 2 and you have done this in your homeworks. If you're unsure how to proceed, revisit the notebooks and copy the relevant code snippets and make the necessary modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n",
    "# (copy/paste from session 2 :-)\n",
    "\n",
    "sst_clim = ...\n",
    "sst_anom = ...\n",
    "sst_anom_detrended = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: To verify that your calculations are correct, you can for example plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional: your plot here. You can maybe plot the std of the anomalies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we select a region in the eastern central Pacific for further analysis. This region is close to the NINO3.4 region, but more to the South-East. This adjustment allows us to compare the results with wind and Ekman parameters later, which isn't feasible directly on the equator.\n",
    "\n",
    "\n",
    "\n",
    "We will slice the data again for latitude and longitude, and then calculate the spatial mean to get a time series for the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the cell\n",
    "lat_start_sst, lat_end_sst = -5, -20\n",
    "lon_start_sst, lon_end_sst = 240, 280  # for 0-360 120°W to 80°W\n",
    "# calculate box-average \n",
    "latitudes = sst_anom_detrended['lat']\n",
    "weights = np.cos(np.deg2rad(latitudes)).where(~sst_anom_detrended[0].isnull()).fillna(0)\n",
    "sst_box = (sst_anom_detrended).sel(lon=slice(lon_start_sst, lon_end_sst), \n",
    "                                   lat=slice(lat_start_sst, lat_end_sst))\n",
    "sst_box_mean = (sst_box.weighted(weights)).mean(dim=['lon', 'lat'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Fourier Transform (FFT)\n",
    "\n",
    "\n",
    "The Fast Fourier Transform (FFT) is a tool for identifying dominant frequencies/periods within a signal -> here the SST anomalies. By transforming these time series into the frequency domain, we can uncover the main cycles that characterize variability in the anomaly data.\n",
    "\n",
    "The power spectrum, obtained by squaring the amplitude of the FFT, reveals the strength of the various frequencies present in the time series. Higher values in the power spectrum indicate a stronger presence of the corresponding frequency in the original time series. By setting a significance threshold—calculated as the mean plus two standard deviations of the power spectrum—we can highlight the most prominent frequencies, filtering them from background noise.\n",
    "\n",
    "Execute the following two code cells to begin the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "# Compute the Fast Fourier Transform (FFT) of the sst_box_mean\n",
    "\n",
    "fft_sst = np.fft.fft(sst_box_mean)\n",
    "\n",
    "n = len(sst_box_mean) # numer of data points\n",
    "# compute corresponding frequency values for the FFT output\n",
    "freq = np.fft.fftfreq(n, d=1)  # with the sampling intervall,  d=1 assumes monthly data points\n",
    "periods = 1 / freq  # Periods will be in months\n",
    "\n",
    "# Compute the Power Spectrum (amplitude squared)\n",
    "power_sst = np.abs(fft_sst)**2\n",
    "\n",
    "# Calculate a simple significance threshold, here set at mean + 2 * standard deviation\n",
    "threshold_sst = np.mean(power_sst) + 2*np.std(power_sst)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the Power Spectrum with Periods on a logarithmic scale\n",
    "valid_periods = periods[:n // 2] > 0  # Exclude zero and negative frequencies (FFT creates symmetric output)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# SST Plot\n",
    "plt.subplot(1, 1, 1)\n",
    "\n",
    "plt.semilogy(periods[:n // 2][valid_periods], power_sst[:n // 2][valid_periods])  # Plot only positive frequencies and their power\n",
    "plt.axhline(y=threshold_sst, color='r', linestyle='--', label='Significance Threshold')\n",
    "plt.title('FFT Power Spectrum of SST Anomalies')\n",
    "plt.xlabel('Period (months)')\n",
    "plt.ylabel('Power (log scale)')\n",
    "plt.grid(True)  # Adding grid lines\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code transforms the time series into the frequency domain using FFT.  \n",
    "It identifies the dominant periods with high power, which are highlighted by comparing them to a significance thresholdand the plot shows the dominant frequencies (but we plot the periods) in the SST anomaly data.  \n",
    "\n",
    "It can be challenging to visually identify the dominant cycles directly from the plot. To make this easier, we will print the dominant periods in months. Run the next cell to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "# Find the indices of significant power values\n",
    "valid_periods = periods[:n // 2] > 0  # Exclude the infinite period at zero frequency\n",
    "significant_sst_indices = np.where(power_sst[:n // 2][valid_periods] > threshold_sst)[0]\n",
    "\n",
    "\n",
    "# Extract the corresponding significant periods\n",
    "significant_sst_periods = periods[:n // 2][valid_periods][significant_sst_indices]\n",
    "\n",
    "\n",
    "# Filter out infinite values from significant_sst_periods\n",
    "significant_sst_periods = significant_sst_periods[np.isfinite(significant_sst_periods)]\n",
    "\n",
    "# Print the significant periods for both time series\n",
    "print(\"Significant SST Periods (months):\", significant_sst_periods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identified dominant periods in our data: approximately 144 months (12 years), 96 months, 48 months, 41 months, 28 and 18 months. These findings correspond to known climatic cycles (e.g. PDO, ENSO etc.)\n",
    "\n",
    "\n",
    "Now that we have identified significant periods in our data, we want to understand how these periods vary over time.\n",
    "\n",
    "# Wavelet Analysis\n",
    "\n",
    "This is where Wavelet Analysis comes into play, offering a method to extract both frequency and time information from a signal simultaneously, unlike FFT, which only offers a global frequency spectrum. This makes it especially useful for analyzing non-stationary signals where frequency content changes over time.\n",
    "\n",
    "\n",
    "\n",
    "### What is Wavelet Analysis?\n",
    "\n",
    "Wavelet analysis breaks a signal into time and frequency components using localized wave-like functions called wavelets. The Morlet wavelet, often used for climate data, effectively captures oscillatory patterns while balancing time and frequency localization.\n",
    "\n",
    "\n",
    "This method is particularly valuable for identifying and understanding climatic cycles and their temporal variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "\n",
    "# time array for 288 months \n",
    "time = np.arange(288)\n",
    "\n",
    "\n",
    "# Compute wavelet transform\n",
    "dt = 1  # Monthly time step\n",
    "mother_wavelet = wavelet.Morlet(6) # standard\n",
    "\n",
    "sst_wavelet, scales, freqs, coi, fft, fftfreqs = wavelet.cwt(sst_box_mean.values, dt, wavelet=mother_wavelet)\n",
    "\n",
    "# Ensure the dimensions of the wavelet match the time series\n",
    "sst_wavelet = sst_wavelet[:, :len(time)]  # Match wavelet data to the time series\n",
    "\n",
    "# Calculate the wavelet power\n",
    "power = np.abs(sst_wavelet) ** 2\n",
    "\n",
    "# Manually calculate significance using chi-square distribution\n",
    "alpha = 0.05\n",
    "dof = 2  # Degrees of freedom for Morlet wavelet\n",
    "signif_level = power.mean() * chi2.ppf(1 - alpha, dof) / dof\n",
    "signif = power / signif_level\n",
    "\n",
    "# Average the power and significance over time for each period\n",
    "mean_power = np.mean(power, axis=1)  # Average power over time for each period\n",
    "mean_significance = np.mean(signif, axis=1)  # Average significance over time for each period\n",
    "\n",
    "# Convert frequencies to periods (in months)\n",
    "periods = 1 / freqs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How Wavelet Analysis Works in the Code**:\n",
    "\n",
    "In the provided wavelet code, we transform the SST anomaly time series into the wavelet domain using the Morlet wavelet, producing a power spectrum that shows the strength of different periods over time. This spectrum is visualized as a contour plot, where color intensity represents signal power at specific periods and times.\n",
    "\n",
    "The Cone of Influence (COI) marks regions where edge effects from the finite time series may distort results. Values inside the COI are more reliable than those near the edges.\n",
    "\n",
    "\n",
    "Significance levels are calculated using the chi-square distribution to identify statistically significant periods and times, shown as contours over the wavelet spectrum. These highlight areas of meaningful variability.\n",
    "\n",
    "**<p style=\"color:royalblue;\">For a step-by-step visual explanation, refer to the notebook  [`wavelet_demo.ipynb`](wavelet_demo.ipynb), providing a clearer understanding of the transformation and interpretation of the results. Please take your time to review it at home.</p>**\n",
    "\n",
    "Next: We plot our Wavelet spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.linspace(2000, 2023, num=288)\n",
    "\n",
    "# Create the figure with subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, gridspec_kw={'width_ratios': [3, 1]}, figsize=(12, 6))\n",
    "\n",
    "# Plot the wavelet power spectrum on the left subplot\n",
    "contour = ax1.contourf(years, periods, np.abs(sst_wavelet), levels=39, extend='both', cmap='jet')\n",
    "\n",
    "# Add significance levels manually\n",
    "ax1.contour(years, periods, signif, levels=[1.0], colors='white', linewidths=2)\n",
    "\n",
    "# Plot the cone of influence (COI) -> indicates region where edge effects make the results less reliable\n",
    "ax1.fill_between(years, coi, periods[-1], color='white', alpha=0.3)\n",
    "\n",
    "# Set the x-axis to show years and y-axis for periods\n",
    "ax1.set_xlabel('Time (years)')\n",
    "ax1.set_ylabel('Period (months)')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_ylim(2, 144)  # Start the y-axis at 2 months to avoid displaying below 2\n",
    "ax1.set_yticks([2, 6, 12, 24, 48, 96, 144, 300])\n",
    "ax1.set_yticklabels(['2', '6', '12', '24', '48', '96', '144','300'])\n",
    "ax1.set_title('Wavelet Spectrum of SST Box Mean Anomalies with COI')\n",
    "\n",
    "# Add color bar to the left subplot\n",
    "cbar = plt.colorbar(contour, ax=ax1, label='Wavelet Power')\n",
    "\n",
    "# Plot the mean power and significance as a function of period on the right subplot\n",
    "ax2.plot(mean_power, periods, 'b', label='Mean Power')\n",
    "#ax2.plot(mean_significance, periods, 'r--', label='Mean Significance')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_ylim(ax1.get_ylim())  # Match the y-axis limits with the left plot\n",
    "ax2.set_ylabel('Period (months)')  # Y-axis now matches the wavelet plot\n",
    "ax2.set_yticks([2, 6, 12, 24, 48, 96, 144, 300])\n",
    "ax2.set_yticklabels(['2', '6', '12', '24', '48', '96', '144', '300'])\n",
    "ax2.set_xlabel('Power (°C²)')\n",
    "ax2.set_title('Mean Power and Significance over Time')\n",
    "\n",
    "# Add grid, legend, and format the plot\n",
    "ax2.grid(True)\n",
    "ax2.legend()\n",
    "ax2.yaxis.set_label_position(\"right\")\n",
    "ax2.yaxis.tick_right()\n",
    "\n",
    "# Show the full plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot demonstrates the presence and evolution of dominant signals in the SST anomalies over time.\n",
    "\n",
    "\n",
    "## Object-Oriented Programming\n",
    "\n",
    "Ok, this was a lot of code – once again we calculated anomalies, detrended data, and applied several analyses. Now, we want to apply the same steps to other datasets, but copying, pasting, and modifying all the code each time isn’t efficient. We’ve already made use of functions and modules, but there’s another tool available to us: **Object-Oriented Programming (OOP)** using classes.\n",
    "\n",
    "OOP organizes code into reusable objects with attributes and methods, which can enhance scalability, modularity, and reusability compared to standalone functions and modules.\n",
    "\n",
    "\n",
    "A class is like a blueprint for a house. It defines the general structure and attributes, such as the number of rooms or the size of the garden, but it’s not a specific house yet. An object is the actual house built from that blueprint. If you remove the kitchen from one specific house (object), other houses remain unaffected. However, if you remove the kitchen from the blueprint (class), no house built from that blueprint will have a kitchen.\n",
    "\n",
    "In the same way, the [`TimeSeriesAnalyzer`](../Modules/timeseries_analyzer.py) class is a blueprint for processing climate data. It defines attributes like the dataset to be analyzed and methods like compute_fft or wavelet_analysis to perform frequency analyses. When we create an instance (object) of this class, such as `processor = TimeSeriesAnalyzer(ds_sst24a)`, it applies these predefined methods to the specific dataset (ds_sst24a). This allows us to analyze various datasets consistently without rewriting the analysis code each time, much like building different houses from the same blueprint.\n",
    "\n",
    "\n",
    "<p style=\"color:royalblue;\">Note that this is a simplified introduction to Object-Oriented Programming (OOP). In practice, we often work with more sophisticated classes, such as those provided by <code>xarray</code>, which we've been using throughout the course to efficiently handle multidimensional climate data.</p>\n",
    "\n",
    "\n",
    "A class consists of attributes, which store data, and methods, which are functions that define behaviors or actions the class can perform.\n",
    "\n",
    "#### Why Classes are Helpful:\n",
    "\n",
    "**Efficiency**: Instead of running individual scripts for different datasets, the class provides a unified framework that can process any dataset with the same structure (daily or monthly resoulution, 3 (time, lat, lon) or 4 dimensions (time, depth, lat, lon)).  \n",
    "    \n",
    "**Modularity**: Methods are self-contained, so debugging and extending functionality is much simpler.\n",
    "    \n",
    "**Reusability**: The class can be reused across different datasets, whether it’s SST, wind data, or other climate variables.\n",
    "\n",
    "**Scalability**: As the analysis grows more complex, we can continue adding methods and functionality to the class without needing to rewrite existing code.\n",
    "\n",
    "Applying the Class to Your Data\n",
    "\n",
    "Let’s now quickly reproduce the calculations we have done before, but this time using the TimeSeriesAnalyzer class to automate the process. Afterward, you’ll get a chance to do the same for the wind data as a major task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "# append the Modules folder to path -> we have done it with the ekman_properties.py module before\n",
    "import sys\n",
    "sys.path.append('../Modules')  \n",
    "\n",
    "# import\n",
    "from timeseries_analyzer import TimeSeriesAnalyzer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the docstring of the `TimeSeriesAnalyzer` Class to get a quick overview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "\n",
    "help(TimeSeriesAnalyzer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "# Create an instance of TimeSeriesAnalyzer with the SST dataset, naming it processor, but feel free to choose any name you prefer.\n",
    "processor = TimeSeriesAnalyzer(ds_sst24a)\n",
    "\n",
    "# Compute anomalies\n",
    "sst_anomalies = processor.compute_anomalies_and_detrend()\n",
    "print(\"Anomalies:\\n\", sst_anomalies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `processor.plot_std_and_annual_var('sst', vmax=5)`, we calculate and plot the standard deviation (variability) of the SST data, along with the annual variability. This gives us a quick visual representation of how the data varies over time and across seasons. The `vmax=5` parameter sets the maximum value of the colorbar, controlling the upper bound for the variability being displayed in the plot. This helps in visually interpreting the intensity of variations across the dataset.\n",
    "\n",
    "Run the next cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "# calculate and plot variability and annual variability\n",
    "processor.plot_std_and_annual_var('sst', vmax=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `processor.compute_fft(variable='sst', lon_min=240, lon_max=280, lat_min=-5, lat_max=-20)`, we compute the Fast Fourier Transform (FFT) of the SST anomalies for the specified spatial region. This process helps us identify the dominant periods in the time series, revealing the most significant cycles present in the SST data over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the cell\n",
    "#Perform FFT analysis\n",
    "processor.compute_fft(variable='sst', lon_min=240, lon_max=280, lat_min=-5, lat_max=-20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `processor.wavelet_analysis(variable='sst', lon_min=240, lon_max=280, lat_min=-5, lat_max=-20)`, we perform a wavelet analysis to examine how the significant frequencies and periods change over time. This method allows us to explore time-localized variations in the SST anomalies, giving us a deeper insight into the temporal evolution of different cycles.\n",
    "\n",
    "Run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "# wavelet analysis\n",
    "processor.wavelet_analysis(variable='sst', lon_min=240, lon_max=280, lat_min=-5, lat_max=-20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just a few lines of code, we were able to recreate and automate all the calculations we performed earlier—from calculating detrended anomalies to performing FFT and wavelet analyses, as well as obtaining a quick overview of the variabilities within each dataset. The class simplifies these processes and makes them easily applicable to different datasets.\n",
    "\n",
    "\n",
    "\n",
    "For a general overview of how object-oriented modeling and programming work, please visit this \n",
    "[Storymap](https://storymaps.arcgis.com/stories/dd9d06f89a63400c96927de117a5b28a). The text is in German, but most browsers offer automatic translation. The article provides a closer look at the programming paradigm and dives into the benefits of encapsulation, inheritance, polymorphism, and abstraction.\n",
    "\n",
    "\n",
    "**3. Exercise**\n",
    "\n",
    "Now it’s your turn to adapt this analysis for the wind data. Before you begin, you need to calculate the Ekman properties (e.g., Ekman transport, Ekman pumping). Do you remember how? Once you’ve calculated these, save them as new variables within your wind dataset.\n",
    "\n",
    "Your task:\n",
    "\n",
    "1. Calculate Ekman properties and store them as new variables within your `ds_wind`. Apply a mask using the variable `number_of_observations`    \n",
    "2. Create an instance of the TimeSeriesAnalyzer class for the wind data.  \n",
    "3. Follow the same analysis steps that we performed for the SST data:\n",
    "    - compute anomalies  \n",
    "    - Compute and plot the variability and annual variability of **vertical Ekman velocity** .  \n",
    "    - Perform an FFT analysis of **vertical Ekman velocity** to determine the dominant periods in the wind data. Choose the same region as SST. Be careful to check the lat and lon dimensions of your wind dataset before starting, as they may differ from the SST data. Adjust the slicing accordingly to match the same region.  \n",
    "    - Conduct a wavelet analysis of **vertical Ekman velocity**  to investigate the temporal evolution of these dominant periods.\n",
    "Finally, compare the dominant frequencies of the SST and the vertical Ekman velocity to see how they align or differ.\n",
    "\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell\n",
    "\n",
    "from ekman_dynamics import compute_ekman_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to calculate Ekman properties and store them in ds_wind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to apply a mask using number_of_observations\n",
    "\n",
    "#'number_of_observations' is the variable used as a mask\n",
    "mask = ...\n",
    "\n",
    "# Apply the mask to the eastward wind data\n",
    "ds_wind = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: plot the temporal mean of a variable to check if everything is correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Create an instance of TimeSeriesAnalyzer with the Wind dataset\n",
    "processor_wind = ...\n",
    "\n",
    "\n",
    "# Compute anomalies\n",
    "wind_anomalies = ...\n",
    "#print(\"Anomalies:\\n\", anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # your code here to calculate and plot std and annual std of vertical Ekman velocity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here: Tip: check lat,lon carefully\n",
    "#Perform FFT analysis to vertical Ekman velocity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here: Tip: check lat,lon carefully\n",
    "# wavelet analysis / vertical Ekman velocity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the power spectra of SST and vertical Ekman velocity from both the FFT and wavelet analyses. Discuss the dominant periods you observe in each dataset, and how they correspond to one another or not. \n",
    "\n",
    "Summarize your findings, highlighting any similarities or differences in the frequency and temporal patterns between the two datasets.\n",
    "\n",
    "\n",
    "\n",
    "Your Observations: \n",
    "\n",
    "....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save files\n",
    "\n",
    "processor.save_results('../Data/SST/SST_Anomalies.nc')\n",
    "processor_wind.save_results('../Data/Wind/Wind_Anomalies.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close files\n",
    "\n",
    "#ds_sst.close()\n",
    "#ds_wind.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra Exercise** \n",
    "\n",
    "Perform the analysis using the longer SST dataset, which spans from 1854 to 2024. You are encouraged to explore various time periods and lengths of the time series. Investigate how different temporal windows affect the identified dominant frequencies, variabilities, and patterns. This will help you understand the impact of dataset length and coverage on your frequency and variability analyses.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## EOF Analysis\n",
    "\n",
    "For a more comprehensive understanding of such variability, longer datasets and a careful examination of different time windows are essential, as they allow us to uncover patterns and signals that might otherwise be missed in shorter or less representative datasets.In addition to the analyses we performed, further investigations can be carried out using advanced techniques like **Empirical Orthogonal Function (EOF) analysis**. EOF analysis is a statistical method used to identify dominant modes of variability in spatiotemporal data. It helps in simplifying the complex relationships within the data by decomposing the dataset into principal components.\n",
    "\n",
    "For a detailed example of how EOF analysis is applied, refer to the accompanying notebook [`EOFanalysis_demo.ipynb`](EOFanalysis_demo.ipynb). This analysis can provide deeper insights into the spatial patterns of variability, complementing the frequency-based methods we've already explored."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
